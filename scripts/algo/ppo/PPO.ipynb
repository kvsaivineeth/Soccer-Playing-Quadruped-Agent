{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false",
    "toc-hr-collapsed": false
   },
   "source": [
    "# PPO Implementation\n",
    "\n",
    "Try to create a basic policy to get the agent to try to kick the ball to the target. The paper for this algorithm can be found [here](https://arxiv.org/pdf/1707.06347.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Setup\n",
    "Hyperparameters and other preliminaries.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training device and dynamically set it to the GPU if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants of the MuJoCo environment. `_c` denotes the *cardinality* or the *count* of the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_walls_c = 3\n",
    "_num_walls = 4\n",
    "_ball_state_c = 9\n",
    "_egocentric_state_c = 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_INPUT_DIM = _walls_c * _num_walls + _ball_state_c + _egocentric_state_c\n",
    "_GAMMA = 0.99  # Discount factor\n",
    "_MINIBATCH_SIZE = 32\n",
    "_LEARNING_RATE = 0.0015\n",
    "_ITERATIONS = 1000000\n",
    "_EPOCHS = 10\n",
    "_MEMORY_SIZE = 10000\n",
    "\n",
    "_HIDDEN_LAYER_1 = 64\n",
    "_HIDDEN_LAYER_2 = 32\n",
    "\n",
    "_SEED = 2019\n",
    "_EPSILON = 0.2  # Probability clip\n",
    "_DROPOUT_PROB = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(_SEED)\n",
    "np.random.seed(_SEED)\n",
    "random.seed(_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define observation and agent inputs\n",
    "\n",
    "Here, an agent observation is converted into the input for TRPO. The observed features that are used are: \n",
    "* Wall vectors for the left, right, top, and back walls of the goal\n",
    "* The ball x,y,z positions and velocicties relative to the agent\n",
    "* The state of the agent itself (joints, etc)\n",
    "\n",
    "The features are converted to be 1-dimensional and then concatenated as follows:\n",
    "$$\\left[ \\matrix{ left \\cr\n",
    "                  right \\cr\n",
    "                  top \\cr\n",
    "                  back \\cr\n",
    "                  ball-state \\cr\n",
    "                  egocentric-state} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_input(obs):\n",
    "  left, right, top, back = obs['goal_walls_positions']\n",
    "  ball_state = obs['ball_state']\n",
    "  egocentric_state = obs['egocentric_state']\n",
    "  \n",
    "  return np.concatenate((\n",
    "    left.ravel(),\n",
    "    right.ravel(),\n",
    "    top.ravel(),\n",
    "    back.ravel(),\n",
    "    ball_state.ravel(),\n",
    "    egocentric_state.ravel()\n",
    "  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(physics):\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_kwargs = {\n",
    "  'reward_func': reward\n",
    "}\n",
    "\n",
    "env = suite.load(domain_name=\"quadruped\", \n",
    "                 task_name=\"soccer\", \n",
    "                 visualize_reward=True, \n",
    "                 task_kwargs=task_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dynamic output required for TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_OUTPUT_DIM = env.action_spec().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "The model is a simple feed foward network with 2 hidden layers. Note that in this Actor-Critic model, the actor tries to fit to the policy and the critic tries to fit to the value function. Additionally, in this case both the actor and the critic share the same subnet to *(hopefully)* converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(PPO, self).__init__()\n",
    "    \n",
    "    self.network_base = nn.Sequential(\n",
    "      nn.Linear(_INPUT_DIM, _HIDDEN_LAYER_1), nn.Dropout(_DROPOUT_PROB), nn.Tanh(),\n",
    "      nn.Linear(_HIDDEN_LAYER_1, _HIDDEN_LAYER_2), nn.Dropout(_DROPOUT_PROB), nn.Tanh(),\n",
    "    )\n",
    "    \n",
    "    self.policy_mu = nn.Linear(_HIDDEN_LAYER_2, _OUTPUT_DIM)\n",
    "    self.policy_log_std = nn.Parameter(torch.zeros(1, _OUTPUT_DIM))\n",
    "    self.value = nn.Linear(_HIDDEN_LAYER_2, 1)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    latent_state = self.network_base(x)\n",
    "    \n",
    "    mus = self.policy_mu(latent_state)\n",
    "    sigma_sq = torch.exp(self.policy_log_std)\n",
    "    value_s = self.value(latent_state)\n",
    "    \n",
    "    return mus, sigma_sq, value_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the network and verify the layers are good as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPO(\n",
       "  (network_base): Sequential(\n",
       "    (0): Linear(in_features=65, out_features=64, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): Tanh()\n",
       "  )\n",
       "  (actor): Linear(in_features=32, out_features=12, bias=True)\n",
       "  (critic): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPO()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "\n",
    "Recall that PPO works based off of trajectories. Create a standarded memory structure so that batches can be sampled from it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transitions = collections.namedtuple('Transition',\n",
    "                                     ['state', 'action', 'reward', 'next_state', 'terminating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create target and stable nets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = PPO().float().to(_DEVICE)\n",
    "policy_old = PPO().float().to(_DEVICE)\n",
    "policy_old.load_state_dict(policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4105, -0.0643,  0.0494, -0.2165,  0.2117,  0.0917, -0.3746, -0.1664,\n",
      "         0.0418, -0.2445, -0.5868, -0.3083])\n"
     ]
    }
   ],
   "source": [
    "for i in range(_ITERATIONS):\n",
    "  timestep = env.reset()\n",
    "  memory = []\n",
    "  \n",
    "  while not timestep.last():\n",
    "    input_ = to_input(timestep.observation)\n",
    "    state = torch.from_numpy(input_).float().to(_DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      action, value = policy_old(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9035, 0.3931, 0.6240, 0.6379, 0.8805, 0.2992, 0.7022, 0.9032, 0.8814,\n",
       "        0.4057, 0.4524, 0.2671, 0.1629, 0.8892, 0.1485, 0.9847, 0.0324, 0.5154,\n",
       "        0.2011, 0.8860, 0.5136, 0.5783, 0.2993, 0.8372, 0.5266, 0.1048, 0.2781,\n",
       "        0.0466, 0.5091, 0.4724, 0.9045, 0.9435, 0.7034, 0.8463, 0.9280, 0.8194,\n",
       "        0.8452, 0.7915, 0.1710, 0.2900, 0.3045, 0.1477, 0.5738, 0.8636, 0.3233,\n",
       "        0.2756, 0.6822, 0.1914, 0.5810, 0.8626, 0.2345, 0.2899, 0.3829, 0.3496,\n",
       "        0.3288, 0.9402, 0.0380, 0.7768, 0.3846, 0.7167, 0.4525, 0.6990, 0.3536,\n",
       "        0.7942, 0.2971])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ = torch.from_numpy(np.random.random(_INPUT_DIM)).float()\n",
    "input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo(input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = env.reset()\n",
    "timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_input(timestep.observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_spec().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
