{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false toc-hr-collapsed=false"
   },
   "source": [
    "### PPO Implementation\n",
    "\n",
    "Try to create a basic policy to get the agent to try to kick the ball to the target. The paper for this algorithm can be found [here](https://arxiv.org/pdf/1707.06347.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false toc-hr-collapsed=false"
   },
   "source": [
    "## Setup\n",
    "Hyperparameters and other preliminaries.\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training device and dynamically set it to the GPU if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "_DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants of the MuJoCo environment. `_c` denotes the *cardinality* or the *count* of the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_walls_c = 3\n",
    "_num_walls = 4\n",
    "_ball_state_c = 9\n",
    "_egocentric_state_c = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DURATION_SEC = 20\n",
    "_step_per_sec = 50\n",
    "_TOTAL_STEPS = _DURATION_SEC * _step_per_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_INPUT_DIM = _walls_c * _num_walls + _ball_state_c + _egocentric_state_c\n",
    "_GAMMA = 0.99  # Discount factor\n",
    "_MINIBATCH_SIZE = 64\n",
    "_LEARNING_RATE = 0.0015\n",
    "_ITERATIONS = 50000\n",
    "_EPOCHS = 15  # Denoted as `K` in the paper\n",
    "\n",
    "_HIDDEN_LAYER_1 = 64\n",
    "_HIDDEN_LAYER_2 = 32\n",
    "\n",
    "_SEED = 2019\n",
    "_EPSILON = 0.2  # Probability clip\n",
    "_VF_C = 1\n",
    "_S_C = 0.01\n",
    "_DROPOUT_PROB = 0.5\n",
    "_MAX_GRAD_NORM = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(_SEED)\n",
    "np.random.seed(_SEED)\n",
    "random.seed(_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "## Define the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define observation and agent inputs\n",
    "\n",
    "Here, an agent observation is converted into the input for TRPO. The observed features that are used are: \n",
    "* Wall vectors for the left, right, top, and back walls of the goal\n",
    "* The ball x,y,z positions and velocicties relative to the agent\n",
    "* The state of the agent itself (joints, etc)\n",
    "\n",
    "The features are converted to be 1-dimensional and then concatenated as follows:\n",
    "$$\\left[ \\matrix{ left \\cr\n",
    "                  right \\cr\n",
    "                  top \\cr\n",
    "                  back \\cr\n",
    "                  ball-state \\cr\n",
    "                  egocentric-state} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_input(obs):\n",
    "  left, right, top, back = obs['goal_walls_positions']\n",
    "  ball_state = obs['ball_state']\n",
    "  egocentric_state = obs['egocentric_state']\n",
    "  \n",
    "  return np.concatenate((\n",
    "    left.ravel(),\n",
    "    right.ravel(),\n",
    "    top.ravel(),\n",
    "    back.ravel(),\n",
    "    ball_state.ravel(),\n",
    "    egocentric_state.ravel()\n",
    "  ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(physics):\n",
    "  arena_size = 2 * physics.named.model.geom_size['floor', 0]\n",
    "\n",
    "  ball_to_goal = physics.ball_to_goal_distance()\n",
    "  agent_to_ball = physics.self_to_ball_distance()\n",
    "  \n",
    "  b2g_scaled = (arena_size - ball_to_goal) / arena_size\n",
    "  a2b_scaled = (arena_size - agent_to_ball)  / arena_size\n",
    "  \n",
    "  return 0.5 - 0.4 * b2g_scaled - 0.1 * a2b_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define termination criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termination(physics):\n",
    "  if physics.ball_in_goal():\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "### Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_kwargs = {\n",
    "  'reward_func': reward,\n",
    "  'termination_func': termination,\n",
    "  'time_limit': float('inf'),\n",
    "}\n",
    "\n",
    "env = suite.load(domain_name=\"quadruped\", \n",
    "                 task_name=\"soccer\", \n",
    "                 visualize_reward=True, \n",
    "                 task_kwargs=task_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dynamic output required for PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_OUTPUT_DIM = env.action_spec().shape[0]\n",
    "_ACTION_MIN = torch.from_numpy(env.action_spec().minimum).float().to(_DEVICE)\n",
    "_ACTION_MAX = torch.from_numpy(env.action_spec().maximum).float().to(_DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "The model is a simple feed foward network with 2 hidden layers. Note that in this Actor-Critic model, the actor tries to fit to the policy and the critic tries to fit to the value function. Additionally, in this case both the actor and the critic share the same subnet to *(hopefully)* converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(PPO, self).__init__()\n",
    "    \n",
    "    self.network_base = nn.Sequential(\n",
    "      nn.Linear(_INPUT_DIM, _HIDDEN_LAYER_1), nn.Dropout(_DROPOUT_PROB), nn.Tanh(),\n",
    "      nn.Linear(_HIDDEN_LAYER_1, _HIDDEN_LAYER_2), nn.Dropout(_DROPOUT_PROB), nn.Tanh(),\n",
    "    )\n",
    "    \n",
    "    self.policy_mu = nn.Linear(_HIDDEN_LAYER_2, _OUTPUT_DIM)\n",
    "    self.policy_log_std = nn.Parameter(torch.randn(_OUTPUT_DIM))\n",
    "    self.value = nn.Linear(_HIDDEN_LAYER_2, 1)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    latent_state = self.network_base(x)\n",
    "    \n",
    "    mus = self.policy_mu(latent_state)\n",
    "    sigmas = torch.exp(self.policy_log_std)\n",
    "    value_s = self.value(latent_state)\n",
    "    \n",
    "    return mus, sigmas, value_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the network and verify the layers are good as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPO(\n",
       "  (network_base): Sequential(\n",
       "    (0): Linear(in_features=65, out_features=64, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Tanh()\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): Tanh()\n",
       "  )\n",
       "  (policy_mu): Linear(in_features=32, out_features=12, bias=True)\n",
       "  (value): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PPO()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=false"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "incorrectly_encoded_metadata": "toc-hr-collapsed=true"
   },
   "source": [
    "### Memory Managment\n",
    "Create structures and methods to help manage the memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration Transition\n",
    "Create a data type to store the transition during exploration. Can't compute advantages and such because the trajectory won't be finished by then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = collections.namedtuple('Transition',\n",
    "                                    ['state',\n",
    "                                     'action',\n",
    "                                     'action_dist',\n",
    "                                     'value',\n",
    "                                     'reward',\n",
    "                                     'mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Memory\n",
    "Create a data to store memories to sample for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Memory = collections.namedtuple('Memory',\n",
    "                                ['state', \n",
    "                                 'action',\n",
    "                                 'action_dist',\n",
    "                                 'value',\n",
    "                                 'value_target',\n",
    "                                 'advantage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation\n",
    "Quickly define a custom dataset so that the trainloader can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, *args):\n",
    "    self.data = args\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    return tuple([d[index] for d in self.data])\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  with torch.no_grad():\n",
    "    states = []\n",
    "    actions = []\n",
    "    action_dists = []\n",
    "    value_targets = []\n",
    "    advantages = []\n",
    "\n",
    "    for s, a, ad, vt, adv in batch:\n",
    "      states.append(s)\n",
    "      actions.append(a)\n",
    "      action_dists.append(ad)\n",
    "      value_targets.append(vt)\n",
    "      advantages.append(adv)\n",
    "\n",
    "    states_tensor = torch.stack(states)\n",
    "    actions_tensor = torch.stack(actions)\n",
    "    value_targ_tensor = torch.stack(value_targets)\n",
    "    adv_tensor = torch.stack(advantages)\n",
    "  \n",
    "  return states_tensor, actions_tensor, action_dists, value_targ_tensor, adv_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for some of the calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_torch = lambda a: torch.from_numpy(np.array(a)).float().to(_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model, memory, optimizer, n_steps=_EPOCHS,\n",
    "                 batch_size=_MINIBATCH_SIZE):\n",
    "  memory = Memory(*zip(*memory))\n",
    "  \n",
    "  # Convert into torch vectors\n",
    "  states_mem = to_torch(memory.state)\n",
    "  actions_mem = to_torch(memory.action)\n",
    "  action_dists_mem = memory.action_dist\n",
    "  value_targets_mem = to_torch(memory.value_target)\n",
    "  \n",
    "  # Advanatages need to be scaled to meet all of the actions\n",
    "  # Not actually that bad because the mean is taken on them later on\n",
    "  advantages_mem = to_torch(memory.advantage)\n",
    "  advantages_mem = advantages_mem.unsqueeze(1).repeat((1, 12))\n",
    "  \n",
    "  dataset = MemoryDataset(states_mem, actions_mem, action_dists_mem, value_targets_mem, \n",
    "                          advantages_mem)\n",
    "  trainloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                                            shuffle=True, collate_fn=collate_fn)\n",
    "  \n",
    "  losses = []\n",
    "  for _ in range(n_steps):\n",
    "    for states, actions, action_dists, value_targets, advantages in trainloader:\n",
    "      # Get predicted actions\n",
    "      mus, sigmas, values = model(states)\n",
    "      predicted_action_dists = torch.distributions.normal.Normal(mus, sigmas)\n",
    "      predicted_actions = predicted_action_dists.sample()\n",
    "      predicted_actions = torch.max(predicted_actions, _ACTION_MIN)\n",
    "      predicted_actions = torch.min(predicted_actions, _ACTION_MAX)\n",
    "\n",
    "      # Convert action distributions into their respective log probabilites\n",
    "      predicted_log_probs = predicted_action_dists.log_prob(predicted_actions)\n",
    "      batch_log_probs = []\n",
    "      for i in range(len(action_dists)):\n",
    "        batch_log_probs.append(action_dists[i].log_prob(actions[i]))\n",
    "      batch_log_probs = torch.stack(batch_log_probs).to(_DEVICE)\n",
    "\n",
    "      # Calculate loss\n",
    "      ratio = torch.exp(predicted_log_probs - batch_log_probs)\n",
    "      surr_1 = ratio * advantages\n",
    "      surr_2 = ratio.clamp(1 - _EPSILON, 1 + _EPSILON) * advantages\n",
    "      loss_clip = torch.mean(torch.min(surr_1, surr_2))\n",
    "      loss_vf = torch.mean((values - value_targets) ** 2)\n",
    "      loss_s = torch.mean(predicted_action_dists.entropy())\n",
    "      loss_total = - (loss_clip - _VF_C * loss_vf + _S_C * loss_s)\n",
    "\n",
    "      # Optimize the model\n",
    "      optimizer.zero_grad()\n",
    "      loss_total.backward()\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), _MAX_GRAD_NORM)\n",
    "      optimizer.step()\n",
    "\n",
    "      # Add to losses\n",
    "      losses.append(loss_total.item())\n",
    "    \n",
    "  return np.array(losses).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration and actually training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create target and stable nets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = PPO().float().to(_DEVICE)\n",
    "policy_old = PPO().float().to(_DEVICE)\n",
    "policy_old.load_state_dict(policy.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(policy.parameters(), lr=_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore, write to memory, and train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 10 loss: 720.0196391028934  reward: 0.18433842601810516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1205 14:18:56.696122 140645952886592 core.py:86] Unknown warning type Time = 0.0000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physics state is invalid. Warning(s) raised: mjWARN_BADCTRL\n",
      "TimeStep(step_type=<StepType.FIRST: 0>, reward=None, discount=None, observation=OrderedDict([('egocentric_state', array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), ('torso_velocity', array([0., 0., 0.])), ('torso_upright', array(1.)), ('imu', array([1.57015265e-16, 1.71031372e-16, 1.77635684e-15, 0.00000000e+00,\n",
      "       0.00000000e+00, 0.00000000e+00])), ('force_torque', array([ 3.32019651e-16,  9.28591164e-16,  5.66608415e-15, -6.61345961e-15,\n",
      "       -3.35318117e-15,  1.46901326e-15,  8.78099583e-15, -9.28591164e-16,\n",
      "       -3.96565306e-15, -5.70457510e-15,  3.35318117e-15,  1.02421884e-16,\n",
      "        8.63101361e-18,  3.08165567e-17, -2.63210841e-18, -3.12528066e-17,\n",
      "        3.70170044e-18,  8.23693006e-18, -8.63101361e-18, -3.21773770e-17,\n",
      "        2.63210841e-18,  3.12528066e-17, -5.74398311e-18, -8.23693006e-18])), ('ball_state', array([-1.43032064, -9.83542888,  1.48      ,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        ])), ('goal_position', array([-9.40597838, -5.05478214, -0.52      ])), ('goal_walls_positions', (array([-7.35510143, -3.62513051,  0.48      ]), array([-11.45685532,  -6.48443376,   0.48      ]), array([-9.40597838, -5.05478214,  1.68      ]), array([-8.7197456 , -6.03920307,  0.48      ])))]))\n",
      "[ -7.35510143  -3.62513051   0.48       -11.45685532  -6.48443376\n",
      "   0.48        -9.40597838  -5.05478214   1.68        -8.7197456\n",
      "  -6.03920307   0.48        -1.43032064  -9.83542888   1.48\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.        ]\n",
      "tensor([ -7.3551,  -3.6251,   0.4800, -11.4569,  -6.4844,   0.4800,  -9.4060,\n",
      "         -5.0548,   1.6800,  -8.7197,  -6.0392,   0.4800,  -1.4303,  -9.8354,\n",
      "          1.4800,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000,   0.0000], device='cuda:0')\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0')\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0')\n",
      "(tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0', grad_fn=<ExpBackward>), tensor([nan], device='cuda:0', grad_fn=<AddBackward0>))\n"
     ]
    },
    {
     "ename": "PhysicsError",
     "evalue": "Physics state is invalid. Warning(s) raised: mjWARN_BADCTRL",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPhysicsError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8538f4551ec8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0mtimestep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/education/wpi/junior/cs525/assignments/project4/env-rl-proj4/lib/python3.6/site-packages/dm_control/rl/control.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_physics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_sub_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_physics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_physics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/education/wpi/junior/cs525/assignments/project4/env-rl-proj4/lib/python3.6/site-packages/dm_control/mujoco/engine.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mmjlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmj_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       \u001b[0mmjlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmj_step1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m   def render(self, height=240, width=320, camera_id=-1, overlays=(),\n",
      "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/education/wpi/junior/cs525/assignments/project4/env-rl-proj4/lib/python3.6/site-packages/dm_control/mujoco/engine.py\u001b[0m in \u001b[0;36mcheck_invalid_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0mwarning_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_warnings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmjtWarning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m       raise _control.PhysicsError(\n\u001b[0;32m--> 274\u001b[0;31m           _INVALID_PHYSICS_STATE.format(warning_names=', '.join(warning_names)))\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPhysicsError\u001b[0m: Physics state is invalid. Warning(s) raised: mjWARN_BADCTRL"
     ]
    }
   ],
   "source": [
    "policy.train() \n",
    "\n",
    "for current_iter in range(_ITERATIONS):\n",
    "  transitions = []\n",
    "  rewards = []\n",
    "  \n",
    "  timestep = env.reset()\n",
    "  \n",
    "  # Explore using the previous policy\n",
    "  episode_length = 0\n",
    "  while not timestep.last() and episode_length <= _TOTAL_STEPS:\n",
    "    input_ = to_input(timestep.observation)\n",
    "    state = torch.from_numpy(input_).float().to(_DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      mus, sigmas, v_s = policy_old(state)\n",
    "      \n",
    "    actions_dist = torch.distributions.normal.Normal(mus, sigmas)\n",
    "    action = actions_dist.sample().cpu().numpy()\n",
    "    action = np.maximum(action, _ACTION_MIN.cpu().numpy())\n",
    "    action = np.minimum(action, _ACTION_MAX.cpu().numpy())\n",
    "    \n",
    "    try:\n",
    "      timestep = env.step(action)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      print(timestep)\n",
    "      print(input_)\n",
    "      print(state)\n",
    "      print(action)\n",
    "      print(mus)\n",
    "      print(sigmas)\n",
    "      print(policy_old(state))\n",
    "      raise\n",
    "    \n",
    "    reward = timestep.discount if timestep.last() else timestep.reward\n",
    "    mask = 1 if timestep.last() else 0\n",
    "    rewards.append(reward)\n",
    "    \n",
    "    transitions.append(Transition(state=input_, action=action, action_dist=actions_dist,\n",
    "                                  value=v_s.item(), mask=mask, reward=reward))\n",
    "    \n",
    "    episode_length += 1\n",
    "  \n",
    "  if episode_length < _MINIBATCH_SIZE * 2:\n",
    "    continue\n",
    "    \n",
    "  # Create the final memory to sample\n",
    "  memory = []\n",
    "  \n",
    "  # Compute advantages using GAE\n",
    "  advantages = []\n",
    "  prev_v_target = prev_v = prev_adv = 0\n",
    "  for trans in reversed(transitions):\n",
    "    # Caculate advantages and proper V(s) values\n",
    "    v_target = trans.reward + _GAMMA * prev_v_target * trans.mask\n",
    "    delta = trans.reward + _GAMMA * prev_v * trans.mask - trans.value\n",
    "    adv = delta + _GAMMA * prev_adv * trans.mask\n",
    "    \n",
    "    # Insert into memory\n",
    "    advantages.insert(0, adv)\n",
    "    memory.insert(0, Memory(\n",
    "      value_target=v_target, advantage=None,  # Replace advantages with standardized advantages\n",
    "      **{k: v for k, v in trans._asdict().items()\n",
    "              if k in set(Memory._fields) & set(Transition._fields)}))\n",
    "    \n",
    "    # Update for the next iteration\n",
    "    prev_v_target = v_target\n",
    "    prev_v = trans.value\n",
    "    prev_adv = adv\n",
    "        \n",
    "  # Normalize advantages\n",
    "  advs = np.array(advantages)\n",
    "  advs = (advs - advs.mean()) / advs.std()\n",
    "  \n",
    "  for t, norm_adv in enumerate(advs):\n",
    "    memory[t] = memory[t]._replace(advantage=norm_adv)\n",
    "    \n",
    "  # Train\n",
    "  loss = update_model(policy, memory, optimizer)\n",
    "  policy_old.load_state_dict(policy.state_dict())\n",
    "  \n",
    "  if current_iter % 10 == 0:\n",
    "    total_rewards = np.array(rewards)\n",
    "    print('iter: {} loss: {}  reward: {}'.format(current_iter,\n",
    "                                                 loss, total_rewards.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_policy(ts):\n",
    "  input_ = to_input(ts.observation)\n",
    "  state = torch.from_numpy(input_).float().to(_DEVICE)\n",
    "    \n",
    "  mus, sigmas, v_s = policy(state)\n",
    "      \n",
    "  actions_dist = torch.distributions.normal.Normal(mus, sigmas)\n",
    "  action = actions_dist.sample().cpu().numpy()\n",
    "  action = np.maximum(action, _ACTION_MIN.cpu().numpy())\n",
    "  action = np.minimum(action, _ACTION_MAX.cpu().numpy())\n",
    "\n",
    "  return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_control import viewer\n",
    "policy.eval()\n",
    "viewer.launch(env, policy=ppo_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
